{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"You have just found Musket Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster Goals and principles Compactness and declarative description Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: [] Reproducibility and ease of sharing As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder. Established way to store and compare results Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling. Flexibility and extensibility Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path Pipelines and IDE Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis. Generic pipeline Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Segmentation Pipeline Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs Classification Pipeline Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Home"},{"location":"#you-have-just-found-musket","text":"Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster","title":"You have just found Musket"},{"location":"#goals-and-principles","text":"","title":"Goals and principles"},{"location":"#compactness-and-declarative-description","text":"Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: []","title":"Compactness and declarative description"},{"location":"#reproducibility-and-ease-of-sharing","text":"As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder.","title":"Reproducibility and ease of sharing"},{"location":"#established-way-to-store-and-compare-results","text":"Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling.","title":"Established way to store and compare results"},{"location":"#flexibility-and-extensibility","text":"Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path","title":"Flexibility and extensibility"},{"location":"#pipelines-and-ide","text":"Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis.","title":"Pipelines and IDE"},{"location":"#generic-pipeline","text":"Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Generic pipeline"},{"location":"#segmentation-pipeline","text":"Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs","title":"Segmentation Pipeline"},{"location":"#classification-pipeline","text":"Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Classification Pipeline"},{"location":"classification/","text":"Classification training pipeline My puny attempt to build reusable training pipeline for image classification Motivation Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library Installation At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install classification_pipeline Note: this package requires python 3.6 Usage guide Training a model Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits. Image Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees Freezing/Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture. Custom datasets You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class Classification: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage. Composite losses Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. Cyclical learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"User guide"},{"location":"classification/#classification-training-pipeline","text":"My puny attempt to build reusable training pipeline for image classification","title":"Classification training pipeline"},{"location":"classification/#motivation","text":"Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library","title":"Motivation"},{"location":"classification/#installation","text":"At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install classification_pipeline Note: this package requires python 3.6","title":"Installation"},{"location":"classification/#usage-guide","text":"","title":"Usage guide"},{"location":"classification/#training-a-model","text":"Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits.","title":"Training a model"},{"location":"classification/#image-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees","title":"Image Augmentations"},{"location":"classification/#freezingunfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing/Unfreezing encoder"},{"location":"classification/#custom-datasets","text":"You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class Classification: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img","title":"Custom datasets"},{"location":"classification/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"classification/#multistage-training","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage.","title":"Multistage training"},{"location":"classification/#composite-losses","text":"Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions.","title":"Composite losses"},{"location":"classification/#cyclical-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300","title":"Cyclical learning rates"},{"location":"classification/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"classification/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"classification/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False)","title":"Using trained model"},{"location":"classification/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"classification/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"classification/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"classification/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"classification/#what-is-supported","text":"At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"classification/#custom-architectures-callbacks-metrics","text":"Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"classification/reference/","text":"","title":"Reference"},{"location":"generic/","text":"Reasons to use Generic Pipeline TODO: add more text from a general promo here Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Installation TODO: make sure this actually works. pip install generic_pipeline Project structure Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. main.py will be always executed, other files require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc. Launching TODO General train properties Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files that are not located in modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Files from the modules folder are imported automatically testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) There are many more properties to check in Reference of root properties Definining networks Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment. Built-in NN layers There are a lot of built-in NN layers, basically, we support all layers that are supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types Control layers Utility layers can be used to set control and data flow inside their bodies. Here are some examples: Simple Data Flow constructions inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1] Repeat and With declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120] Conditional layers declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid] Shared Weights #Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations Wrapper layers net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid] Manually controlling data flow net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here Datasets Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities. Callbacks Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here Stages stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Preprocessors Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min]) How to check training results In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"User guide"},{"location":"generic/#reasons-to-use-generic-pipeline","text":"TODO: add more text from a general promo here Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Reasons to use Generic Pipeline"},{"location":"generic/#installation","text":"TODO: make sure this actually works. pip install generic_pipeline","title":"Installation"},{"location":"generic/#project-structure","text":"Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. main.py will be always executed, other files require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc.","title":"Project structure"},{"location":"generic/#launching","text":"TODO","title":"Launching"},{"location":"generic/#general-train-properties","text":"Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files that are not located in modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Files from the modules folder are imported automatically testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) There are many more properties to check in Reference of root properties","title":"General train properties"},{"location":"generic/#definining-networks","text":"Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment.","title":"Definining networks"},{"location":"generic/#built-in-nn-layers","text":"There are a lot of built-in NN layers, basically, we support all layers that are supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types","title":"Built-in NN layers"},{"location":"generic/#control-layers","text":"Utility layers can be used to set control and data flow inside their bodies. Here are some examples:","title":"Control layers"},{"location":"generic/#simple-data-flow-constructions","text":"inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1]","title":"Simple Data Flow constructions"},{"location":"generic/#repeat-and-with","text":"declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120]","title":"Repeat and With"},{"location":"generic/#conditional-layers","text":"declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid]","title":"Conditional layers"},{"location":"generic/#shared-weights","text":"#Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations","title":"Shared Weights"},{"location":"generic/#wrapper-layers","text":"net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid]","title":"Wrapper layers"},{"location":"generic/#manually-controlling-data-flow","text":"net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here","title":"Manually controlling data flow"},{"location":"generic/#datasets","text":"Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities.","title":"Datasets"},{"location":"generic/#callbacks","text":"Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here","title":"Callbacks"},{"location":"generic/#stages","text":"stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Stages"},{"location":"generic/#preprocessors","text":"Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min])","title":"Preprocessors"},{"location":"generic/#how-to-check-training-results","text":"In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"How to check training results"},{"location":"generic/reference/","text":"Generic pipeline reference Pipeline root properties activation TODO: does it have any use in the root of the file? aggregation_metric type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout architecture type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork augmentation type : ```` TODO: isnt that a property from segmentation/classification, not generic pipeline? type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 batch type : integer Sets up training batch size. Example: batch: 512 classes type : ```` TODO: does it have any use in generic pipeline? Example: callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] dataset_augmenter type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. TODO: check that description and example are correct? Example: dataset_augmenter: name: TheAugmenter parameter: test dropout type : float TODO: does it have any use in generic pipeline root? Example: declarations type : complex Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"] extra_train_data type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. TODO is that correct? Example: folds_count type : integer Number of folds to train. Default is 5. Example: freeze_encoder type : boolean Whether to freeze encoder during the training process. TODO isnt it non-generic property? Example: final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] holdout type : ```` TODO what is this? Example: imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : integer Size of batch during inferring process. Example: loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: testTimeAugmentation type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example: transforms type : complex TODO is that true? If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example: Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] TODO: examples from lr_variation_callback.py look strange, also it is unclear how to number of steps is being set up. Example TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Layer types Input TODO: description Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example: GaussianNoise Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example: Dropout Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. seed - integer; integer to use as random seed Example: declarations: net: - dropout: 0.5 SpatialDropout1D Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example: LSTM Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Activation function to use for the recurrent step (see activations ). Default: hard sigmoid ( hard_sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. Example: GlobalMaxPool1D Global max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example: GlobalAveragePooling1D Global average pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example: BatchNormalization Batch normalization layer. Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format=\"channels_first\" , set axis=1 in BatchNormalization . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Example: Concatenate Layer that concatenates a list of inputs. Example: - concatenate: [lstmBranch,textFeatureBranch] Add Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - add: [first,second] Substract ayer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Example: - substract: [first,second] Mult Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - mult: [first,second] Max Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - max: [first,second] Min Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - min: [first,second] Conv1D 1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", or (None, 128) for variable-length sequences with 128 features per step. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: Conv2D 2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: MaxPool1D Max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example: MaxPool2D Max pooling operation for spatial data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example: AveragePooling1D Average pooling for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example: CuDNNLSTM Fast LSTM implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example: Dense Regular densely-connected NN layer. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: Flatten Flattens the input. Does not affect the batch size. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example: Bidirectional Bidirectional wrapper for RNNs. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Example: Utility layers split Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Number of outputs is equal to a number of children. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-concat Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128] split-concatenate Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows (equal to the usage of Concatenate layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128] split-add Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is an addition of child flows (equal to the usage of Add layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-substract Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a substraction of child flows (equal to the usage of Substract layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-mult Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a multiplication of child flows (equal to the usage of Mult layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-min Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a minimum of child flows (equal to the usage of Min layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-max Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a maximum of child flows (equal to the usage of Max layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-dot Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-dot-normalize Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product with normalization of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: seq Executes child elements as a sequence of operations, one by one. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input Overrides current input with what is listed. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input: [firstRef, secondRef] pass Stops execution of this branch and drops its output. TODO check that description is correct. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: transform-concat TODO Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-add TODO Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Stage properties loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: initial_weights type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights epochs type : integer Number of epochs to train for this stage. Example: unfreeze_encoder TODO is this for generic? callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 extra_callbacks TODO Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. TODO what for? Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example:","title":"Reference"},{"location":"generic/reference/#generic-pipeline-reference","text":"","title":"Generic pipeline reference"},{"location":"generic/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"generic/reference/#activation","text":"TODO: does it have any use in the root of the file?","title":"activation"},{"location":"generic/reference/#aggregation_metric","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout","title":"aggregation_metric"},{"location":"generic/reference/#architecture","text":"type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork","title":"architecture"},{"location":"generic/reference/#augmentation","text":"type : ```` TODO: isnt that a property from segmentation/classification, not generic pipeline? type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"augmentation"},{"location":"generic/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 512","title":"batch"},{"location":"generic/reference/#classes","text":"type : ```` TODO: does it have any use in generic pipeline? Example:","title":"classes"},{"location":"generic/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"generic/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"generic/reference/#clipnorm","text":"type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"generic/reference/#clipvalue","text":"type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"generic/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"generic/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"generic/reference/#dataset_augmenter","text":"type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. TODO: check that description and example are correct? Example: dataset_augmenter: name: TheAugmenter parameter: test","title":"dataset_augmenter"},{"location":"generic/reference/#dropout","text":"type : float TODO: does it have any use in generic pipeline root? Example:","title":"dropout"},{"location":"generic/reference/#declarations","text":"type : complex Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"]","title":"declarations"},{"location":"generic/reference/#extra_train_data","text":"type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. TODO is that correct? Example:","title":"extra_train_data"},{"location":"generic/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example:","title":"folds_count"},{"location":"generic/reference/#freeze_encoder","text":"type : boolean Whether to freeze encoder during the training process. TODO isnt it non-generic property? Example:","title":"freeze_encoder"},{"location":"generic/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"generic/reference/#holdout","text":"type : ```` TODO what is this? Example:","title":"holdout"},{"location":"generic/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"generic/reference/#inference_batch","text":"type : integer Size of batch during inferring process. Example:","title":"inference_batch"},{"location":"generic/reference/#loss","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"generic/reference/#lr","text":"type : float Learning rate. Example:","title":"lr"},{"location":"generic/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"generic/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"generic/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"generic/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"generic/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"generic/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"generic/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"generic/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"generic/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"generic/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"generic/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"generic/reference/#testtimeaugmentation","text":"type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example:","title":"testTimeAugmentation"},{"location":"generic/reference/#transforms","text":"type : complex TODO is that true? If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"transforms"},{"location":"generic/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example:","title":"validationSplit"},{"location":"generic/reference/#callback-types","text":"","title":"Callback types"},{"location":"generic/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"generic/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"generic/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"generic/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] TODO: examples from lr_variation_callback.py look strange, also it is unclear how to number of steps is being set up. Example","title":"LRVariator"},{"location":"generic/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"generic/reference/#layer-types","text":"","title":"Layer types"},{"location":"generic/reference/#input","text":"TODO: description Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example:","title":"Input"},{"location":"generic/reference/#gaussiannoise","text":"Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example:","title":"GaussianNoise"},{"location":"generic/reference/#dropout_1","text":"Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. seed - integer; integer to use as random seed Example: declarations: net: - dropout: 0.5","title":"Dropout"},{"location":"generic/reference/#spatialdropout1d","text":"Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example:","title":"SpatialDropout1D"},{"location":"generic/reference/#lstm","text":"Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Activation function to use for the recurrent step (see activations ). Default: hard sigmoid ( hard_sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. Example:","title":"LSTM"},{"location":"generic/reference/#globalmaxpool1d","text":"Global max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example:","title":"GlobalMaxPool1D"},{"location":"generic/reference/#globalaveragepooling1d","text":"Global average pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example:","title":"GlobalAveragePooling1D"},{"location":"generic/reference/#batchnormalization","text":"Batch normalization layer. Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format=\"channels_first\" , set axis=1 in BatchNormalization . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Example:","title":"BatchNormalization"},{"location":"generic/reference/#concatenate","text":"Layer that concatenates a list of inputs. Example: - concatenate: [lstmBranch,textFeatureBranch]","title":"Concatenate"},{"location":"generic/reference/#add","text":"Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - add: [first,second]","title":"Add"},{"location":"generic/reference/#substract","text":"ayer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Example: - substract: [first,second]","title":"Substract"},{"location":"generic/reference/#mult","text":"Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - mult: [first,second]","title":"Mult"},{"location":"generic/reference/#max","text":"Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - max: [first,second]","title":"Max"},{"location":"generic/reference/#min","text":"Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - min: [first,second]","title":"Min"},{"location":"generic/reference/#conv1d","text":"1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", or (None, 128) for variable-length sequences with 128 features per step. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Conv1D"},{"location":"generic/reference/#conv2d","text":"2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Conv2D"},{"location":"generic/reference/#maxpool1d","text":"Max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example:","title":"MaxPool1D"},{"location":"generic/reference/#maxpool2d","text":"Max pooling operation for spatial data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example:","title":"MaxPool2D"},{"location":"generic/reference/#averagepooling1d","text":"Average pooling for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example:","title":"AveragePooling1D"},{"location":"generic/reference/#cudnnlstm","text":"Fast LSTM implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example:","title":"CuDNNLSTM"},{"location":"generic/reference/#dense","text":"Regular densely-connected NN layer. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Dense"},{"location":"generic/reference/#flatten","text":"Flattens the input. Does not affect the batch size. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example:","title":"Flatten"},{"location":"generic/reference/#bidirectional","text":"Bidirectional wrapper for RNNs. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Example:","title":"Bidirectional"},{"location":"generic/reference/#utility-layers","text":"","title":"Utility layers"},{"location":"generic/reference/#split","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Number of outputs is equal to a number of children. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split"},{"location":"generic/reference/#split-concat","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128]","title":"split-concat"},{"location":"generic/reference/#split-concatenate","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows (equal to the usage of Concatenate layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128]","title":"split-concatenate"},{"location":"generic/reference/#split-add","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is an addition of child flows (equal to the usage of Add layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-add"},{"location":"generic/reference/#split-substract","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a substraction of child flows (equal to the usage of Substract layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-substract"},{"location":"generic/reference/#split-mult","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a multiplication of child flows (equal to the usage of Mult layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-mult"},{"location":"generic/reference/#split-min","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a minimum of child flows (equal to the usage of Min layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-min"},{"location":"generic/reference/#split-max","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a maximum of child flows (equal to the usage of Max layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-max"},{"location":"generic/reference/#split-dot","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-dot"},{"location":"generic/reference/#split-dot-normalize","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product with normalization of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-dot-normalize"},{"location":"generic/reference/#seq","text":"Executes child elements as a sequence of operations, one by one. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"seq"},{"location":"generic/reference/#input_1","text":"Overrides current input with what is listed. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input: [firstRef, secondRef]","title":"input"},{"location":"generic/reference/#pass","text":"Stops execution of this branch and drops its output. TODO check that description is correct. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"pass"},{"location":"generic/reference/#transform-concat","text":"TODO Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"transform-concat"},{"location":"generic/reference/#transform-add","text":"TODO Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"transform-add"},{"location":"generic/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"generic/reference/#loss_1","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"generic/reference/#lr_1","text":"type : float Learning rate. Example:","title":"lr"},{"location":"generic/reference/#initial_weights","text":"type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights","title":"initial_weights"},{"location":"generic/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"generic/reference/#unfreeze_encoder","text":"TODO is this for generic?","title":"unfreeze_encoder"},{"location":"generic/reference/#callbacks_1","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"generic/reference/#extra_callbacks","text":"TODO","title":"extra_callbacks"},{"location":"generic/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"generic/reference/#cache","text":"Caches its input. TODO what for? Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"cache"},{"location":"generic/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"generic/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"generic/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"generic/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"generic/reference/#augmentation_1","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example:","title":"augmentation"},{"location":"segmentation/","text":"Segmentation Training Pipeline Motivation Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library Installation At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install segmentation_pipeline Note: this package requires python 3.6 Usage guide Training a model Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Image and Mask Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees Freezing and Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture. Custom datasets Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used in my experiments with Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE: def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5) Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage. Composite losses Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. Cyclical learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Background Augmenter One interesting augentation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model. Examples Training background removal task(Pics Art Hackaton) in google collab FAQ How to continue training after crash? If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds) My notebooks constantly run out of memory, what can I do to reduce memory usage? One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3 How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter? BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5] How can I visualize images that are used for training (after augmentations)? You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch What I can do if i have some extra training data, that should not be included into validation, but should be used during the training? extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people How to get basic statistics across my folds/stages This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info() I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage? There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real What if I would like to build a really large ansemble of models? One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d) How to train on multiple gpus? cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"User guide"},{"location":"segmentation/#segmentation-training-pipeline","text":"","title":"Segmentation Training Pipeline"},{"location":"segmentation/#motivation","text":"Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library","title":"Motivation"},{"location":"segmentation/#installation","text":"At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install segmentation_pipeline Note: this package requires python 3.6","title":"Installation"},{"location":"segmentation/#usage-guide","text":"","title":"Usage guide"},{"location":"segmentation/#training-a-model","text":"Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically.","title":"Training a model"},{"location":"segmentation/#image-and-mask-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees","title":"Image and Mask Augmentations"},{"location":"segmentation/#freezing-and-unfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing and Unfreezing encoder"},{"location":"segmentation/#custom-datasets","text":"Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used in my experiments with Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE: def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5)","title":"Custom datasets"},{"location":"segmentation/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"segmentation/#multistage-training","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage.","title":"Multistage training"},{"location":"segmentation/#composite-losses","text":"Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions.","title":"Composite losses"},{"location":"segmentation/#cyclical-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300","title":"Cyclical learning rates"},{"location":"segmentation/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"segmentation/#background-augmenter","text":"One interesting augentation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve","title":"Background Augmenter"},{"location":"segmentation/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"segmentation/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False)","title":"Using trained model"},{"location":"segmentation/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"segmentation/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"segmentation/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"segmentation/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"segmentation/#what-is-supported","text":"At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"segmentation/#custom-architectures-callbacks-metrics","text":"Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"segmentation/#examples","text":"Training background removal task(Pics Art Hackaton) in google collab","title":"Examples"},{"location":"segmentation/#faq","text":"","title":"FAQ"},{"location":"segmentation/#how-to-continue-training-after-crash","text":"If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds)","title":"How to continue training after crash?"},{"location":"segmentation/#my-notebooks-constantly-run-out-of-memory-what-can-i-do-to-reduce-memory-usage","text":"One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3","title":"My notebooks constantly run out of memory, what can I do to reduce memory usage?"},{"location":"segmentation/#how-can-i-run-sepate-set-of-augmenters-on-initial-imagemask-when-replacing-backgrounds-with-background-augmenter","text":"BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5]","title":"How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter?"},{"location":"segmentation/#how-can-i-visualize-images-that-are-used-for-training-after-augmentations","text":"You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch","title":"How can I visualize images that are used for training (after augmentations)?"},{"location":"segmentation/#what-i-can-do-if-i-have-some-extra-training-data-that-should-not-be-included-into-validation-but-should-be-used-during-the-training","text":"extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people","title":"What I can do if i have some extra training data, that should not be included into validation, but should be used during the training?"},{"location":"segmentation/#how-to-get-basic-statistics-across-my-foldsstages","text":"This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info()","title":"How to get basic statistics across my folds/stages"},{"location":"segmentation/#i-have-some-callbacks-that-are-configured-globally-but-i-need-some-extra-callbacks-for-my-last-training-stage","text":"There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real","title":"I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage?"},{"location":"segmentation/#what-if-i-would-like-to-build-a-really-large-ansemble-of-models","text":"One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d)","title":"What if I would like to build a really large ansemble of models?"},{"location":"segmentation/#how-to-train-on-multiple-gpus","text":"cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"How to train on multiple gpus?"},{"location":"segmentation/reference/","text":"Segmentation pipeline reference Pipeline root properties activation type : string Activation function that should be used in last layer. In the case of binary segmentation it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid aggregation_metric type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout architecture type : string This property configures decoder architecture that should be used: At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN augmentation type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 backbone type : string This property configures encoder that should be used: FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : batch type : integer Sets up training batch size. Example: batch: 512 classifier type : string TODO description Supported values: - ResNet - resnet50 DenseNet - densenet121 - densenet169 - densenet201 Example: classifier_lr type : float TODO description Example: classes type : integer Number of classes that should be segmented. Example: callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 compressPredictionsAsInts type : boolean Whether to represent predictions as integers (up to 4 channels) TODO check this is correct Example: compressPredictionsAsInts: true copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 crops type : integer Number of crops to make from original image. Example: dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] dataset_augmenter type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test dropout type : float Example: encoder_weights type : string This property configures initial weights of the encoder, supported values: imagenet Example: encoder_weights: imagenet extra_train_data type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example: folds_count type : integer Number of folds to train. Default is 5. Example: freeze_encoder type : boolean Whether to freeze encoder during the training process. Example: final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] holdout type : ```` Example: imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : integer Size of batch during inferring process. Example: loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: manualResize type : boolean TODO Example: metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: shape type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3] stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: testTimeAugmentation type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example: transforms type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example: Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Stage properties loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: initial_weights type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights epochs type : integer Number of epochs to train for this stage. Example: unfreeze_encoder callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 extra_callbacks Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example:","title":"Reference"},{"location":"segmentation/reference/#segmentation-pipeline-reference","text":"","title":"Segmentation pipeline reference"},{"location":"segmentation/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"segmentation/reference/#activation","text":"type : string Activation function that should be used in last layer. In the case of binary segmentation it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid","title":"activation"},{"location":"segmentation/reference/#aggregation_metric","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout","title":"aggregation_metric"},{"location":"segmentation/reference/#architecture","text":"type : string This property configures decoder architecture that should be used: At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN","title":"architecture"},{"location":"segmentation/reference/#augmentation","text":"type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"augmentation"},{"location":"segmentation/reference/#backbone","text":"type : string This property configures encoder that should be used: FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC :","title":"backbone"},{"location":"segmentation/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 512","title":"batch"},{"location":"segmentation/reference/#classifier","text":"type : string TODO description Supported values: - ResNet - resnet50 DenseNet - densenet121 - densenet169 - densenet201 Example:","title":"classifier"},{"location":"segmentation/reference/#classifier_lr","text":"type : float TODO description Example:","title":"classifier_lr"},{"location":"segmentation/reference/#classes","text":"type : integer Number of classes that should be segmented. Example:","title":"classes"},{"location":"segmentation/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"segmentation/reference/#compresspredictionsasints","text":"type : boolean Whether to represent predictions as integers (up to 4 channels) TODO check this is correct Example: compressPredictionsAsInts: true","title":"compressPredictionsAsInts"},{"location":"segmentation/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"segmentation/reference/#clipnorm","text":"type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"segmentation/reference/#clipvalue","text":"type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"segmentation/reference/#crops","text":"type : integer Number of crops to make from original image. Example:","title":"crops"},{"location":"segmentation/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"segmentation/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"segmentation/reference/#dataset_augmenter","text":"type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test","title":"dataset_augmenter"},{"location":"segmentation/reference/#dropout","text":"type : float Example:","title":"dropout"},{"location":"segmentation/reference/#encoder_weights","text":"type : string This property configures initial weights of the encoder, supported values: imagenet Example: encoder_weights: imagenet","title":"encoder_weights"},{"location":"segmentation/reference/#extra_train_data","text":"type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example:","title":"extra_train_data"},{"location":"segmentation/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example:","title":"folds_count"},{"location":"segmentation/reference/#freeze_encoder","text":"type : boolean Whether to freeze encoder during the training process. Example:","title":"freeze_encoder"},{"location":"segmentation/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"segmentation/reference/#holdout","text":"type : ```` Example:","title":"holdout"},{"location":"segmentation/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"segmentation/reference/#inference_batch","text":"type : integer Size of batch during inferring process. Example:","title":"inference_batch"},{"location":"segmentation/reference/#loss","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"segmentation/reference/#lr","text":"type : float Learning rate. Example:","title":"lr"},{"location":"segmentation/reference/#manualresize","text":"type : boolean TODO Example:","title":"manualResize"},{"location":"segmentation/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"segmentation/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"segmentation/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"segmentation/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"segmentation/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"segmentation/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"segmentation/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"segmentation/reference/#shape","text":"type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3]","title":"shape"},{"location":"segmentation/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"segmentation/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"segmentation/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"segmentation/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"segmentation/reference/#testtimeaugmentation","text":"type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example:","title":"testTimeAugmentation"},{"location":"segmentation/reference/#transforms","text":"type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"transforms"},{"location":"segmentation/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example:","title":"validationSplit"},{"location":"segmentation/reference/#callback-types","text":"","title":"Callback types"},{"location":"segmentation/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"segmentation/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"segmentation/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"segmentation/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example","title":"LRVariator"},{"location":"segmentation/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"segmentation/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"segmentation/reference/#loss_1","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"segmentation/reference/#lr_1","text":"type : float Learning rate. Example:","title":"lr"},{"location":"segmentation/reference/#initial_weights","text":"type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights","title":"initial_weights"},{"location":"segmentation/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"segmentation/reference/#unfreeze_encoder","text":"","title":"unfreeze_encoder"},{"location":"segmentation/reference/#callbacks_1","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"segmentation/reference/#extra_callbacks","text":"","title":"extra_callbacks"},{"location":"segmentation/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"segmentation/reference/#cache","text":"Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"cache"},{"location":"segmentation/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"segmentation/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"segmentation/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"segmentation/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"segmentation/reference/#augmentation_1","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example:","title":"augmentation"}]}